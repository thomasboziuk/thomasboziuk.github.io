<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision on Boziuk</title>
    <link>http://www.boziuk.com/tags/computer-vision/</link>
    <description>Recent content in Computer Vision on Boziuk</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Thomas Boziuk</copyright>
    
	<atom:link href="http://www.boziuk.com/tags/computer-vision/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Dance to (make) the music!</title>
      <link>http://www.boziuk.com/project/dance-to-make-the-music/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>http://www.boziuk.com/project/dance-to-make-the-music/</guid>
      <description>Video Demo: https://youtu.be/rOtRAhowq1Y My final project for CS50x was a python-based application that used computer vision to dynamically generate sound based on the camera subject’s movements. The GUI The GUI, which is relatively simple, uses the tkinter package. The GUI is used to display the camera’s output, as well as including a selector widget to modify the MIDI instrument being used to render the sounds (which required a simple interrupt function to be written) and a slider widget to adjust the sensitivity of the percussion to the subject’s wrist motions, as well as a stop button to end the program and free the camera and virtual MIDI device.</description>
    </item>
    
  </channel>
</rss>